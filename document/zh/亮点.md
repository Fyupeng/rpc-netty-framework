<a id="length">吃药休息</a>
### <a id="informationDigestAlgorithm">1. 信息摘要算法的应用</a>
对于信息摘要算法的使用，其实并不难，在数据包中添加 `String` 类型的成员变量 `checkCode` 用来防伪的就可以实现。
- 原理

发送端把原信息用`HASH`函数加密成摘要,然后把数字摘要和原信息一起发送到接收端,接收端也用HASH函数把原消息加密为摘要,看两个摘要是否相同,若相同,则表明信息的完整.否则不完整。
- 实现

客户端在发出 请求包，服务端会在该请求包在请求执行的结果的内容转成字节码后，使用 MD5 单向加密成为 唯一的信息摘要（128 比特，16 字节）存储到响应包对应的成员变量 `checkCode` 中，所以客户端拿到响应包后，最有利用价值的地方（请求要执行的结果被改动），那么 `checkCode` 将不能保证一致性，这就是信息摘要的原理应用。

**安全性再增强**

考虑到这只是针对客户需求的结果返回一致性，并不能确保请求包之间存在相同的请求内容，所以引入了请求 `id`。

每个包都会生成唯一的 `requestId`，发出请求包后，该包只能由该请求发出的客户端所接受，就算两处有一点被对方恶意改动了，客户端都会报错并丢弃收到的响应包，不会拆包后去返回给用户。

如果不是单单改动了返回结果，而是将结果跟信息摘要都修改了，对方很难保证修改的内容加密后与修改后的信息摘要一致，因为要保证一致的数据传输协议和数据编解码。

---- 

### 2. 心跳机制

心跳机制的 `RPC` 上应用的很广泛，本项目对心跳机制的实现很简单，而且应对措施是服务端强制断开连接，当然有些 `RPC` 框架实现了服务端去主动尝试重连。
- 原理

对于心跳机制的应用，其实是使用了 `Netty` 框架中的一个 `handler` 处理器，通过该 处理器，去定时发送心跳包，让服务端知道该客户端保持活性状态。

- 实现

利用了 `Netty` 框架中的 `IdleStateEvent` 事件监听器，重写`userEventTriggered()` 方法，在服务端监听读操作，读取客户端的 写操作，在客户端监听写操作，监听本身是否还在活动，即有没有向服务端发送请求。

如果客户端没有主动断开与服务端的连接，而继续保持连接着，那么客户端的写操作超时后，也就是客户端的监听器监听到客户端没有的规定时间内做出写操作事件，那么这时客户端该处理器主动发送心跳包给服务端，保证客户端让服务端确保自己保持着活性。

---- 

### 3. SPI 机制

资源目录`META-INF/services`下新建接口全限定名作为文件名，内容为实现类全限定名，支持`JDK`内置`SPI`。

本质通过反射来无参构造创建实例，如果构造函数涉及到通过参数来实现注入成员，那么可将接口转为抽象类，抽象类暴露set方法来让子类重写，从而间接实现注入。

该机制将注册中心逻辑层处理服务发现和注册的接口时实现分离到配置文件`META-INF/services`，从而更好地去支持其他插件，如`Zookeeper`、`Eureka`的扩展。

应用到的配置文件：
- `cn.fyupeng.discovery.ServiceDiscovery`

```properties
cn.fyupeng.discovery.NacosServiceDiscovery
```
- `cn.fyupeng.provider.ServiceProvider`

```properties
cn.fyupeng.provider.DefaultServiceProvider
```
- `cn.fyupeng.registry.ServiceRegistry`
```properties
cn.fyupeng.registry.NacosServiceRegistry
```

---- 

### 4. IO 异步非阻塞

IO 异步非阻塞 能够让客户端在请求数据时处于阻塞状态，而且能够在请求数据返回时间段里去处理自己感兴趣的事情。

- 原理

使用 java8 出世的 `CompletableFuture` 并发工具类，能够异步处理数据，并在将来需要时获取。


- 实现

数据在服务端与客户端之间的通道 `channel` 中传输，客户端向通道发出请求包，需要等待服务端返回，这时可使用 `CompletableFuture` 作为返回结果，只需让客户端读取到数据后，将结果通过 `complete()`方法将值放进去后，在将来时通过`get()`方法获取结果。

---- 

### 5. RNF 协议

- 定义

```java
/**
     * 自定义对象头 协议 8 字节
     * 2 字节 魔数
     * 1 字节 协议包类型
     * 1 字节 序列化类型
     * 4 字节 数据长度
     *
     *       The transmission protocol is as follows :
     * +---------------+---------------+-----------------+-------------+
     * | Magic Number  | Package Type  | Serializer Type | Data Length |
     * | 2 bytes       | 1 bytes       | 1 bytes         | 4 bytes     |
     * +---------------+---------------+-----------------+-------------+
     * |                           Data Bytes                          |
     * |                       Length: ${Data Length}                  |
     * +---------------+---------------+-----------------+-------------+
     */
```

`RNF`协议为上层应用协议，处于应用层中，`TCP`协议为传输协议，即上层传输有`TCP`拆包成`RNF`包，下层传输为`RNF`包封装成`TCP`包。

- 拆解分析

```postgresql
Frame 30759: 368 bytes on wire (2944 bits), 368 bytes captured (2944 bits) on interface \Device\NPF_Loopback, id 0
Null/Loopback
Internet Protocol Version 4, Src: 192.168.2.185, Dst: 192.168.2.185
Transmission Control Protocol, Src Port: 53479, Dst Port: 8085, Seq: 4861, Ack: 4486, Len: 324
RNF Protocol
    Identifier: 0xcafebabe
    PackType: 726571
    SerializerCode: 2
    Length: 308
    Data [truncated]: C\036cn.fyupeng.protocol.RpcRequest�\trequestId\rinterfaceName\nmethodName\nreturnType\005group\theartBeat\nparameters\nparamTypes`\025230113199152359542784\fhelloService\bsayHelloC\017java.lang.Class�\004namea\036cn
```

`Identifier` (`0xcafebabe`): 表示标识，而0xCAFEBABE为java对象的对象头表示。

`PackType` (`726571`, `726573`): `726571`由字符`res`的`ASCCI`码(`\u0072\u0065\u0073`)转换而来，`req`同理。

`SerializerCode` (`0,1,2`): 目前提供三种(`json,kryo,hessian2`)序列化方式，分别对应(`0,1,2`)。

`Length` (`308`): 表示`Data`的长度。

`Data [truncated]`(``C\036cn.fyupeng.protocol.RpcRequest...``): 表示使用`${SerializerCode}`序列化方式、长度为`${Length}`的`${PackType}`包。

使用`Wireshare`本地连接抓取，并用`lua`语言编写`RNF`协议解码器，可以更直观了解包的层次结构。

想了解解码器的可以从根目录直接下载，放于`Wireshark`的`lua`插件`plugins`中，重新加载插件就可以了。

----

### 6. 场景应用

- 支持 springBoot 集成

为了支持`springBoot`集成`logback`日志，继承`rpc-netty-framework`使用同一套日志，抛弃`nacos-client`内置的`slf4j-api`与`commons-loging`原有`Jar`包，因为该框架会导致在整合`springboot`时，出现重复的日志绑定和日志打印方法的参数兼容问题，使用`jcl-over-slf4j-api`可解决该问题；

在`springboot1.0`和`2.0`版本中，不使用它默认版本的`spring-boot-starter-log4j`,推荐使用`1.3.8.RELEASE`；

在场景测试下，突破万字文章的

springboot简单配置如下
```xml
<dependencies>
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter</artifactId>
        <exclusions>
            <!-- 排除 springboot 默认的 logback 日志框架 -->
            <exclusion>
                <groupId>org.springframework.boot</groupId>
                <artifactId>spring-boot-starter-logging</artifactId>
            </exclusion>
            <!-- 排除 springboot 默认的 commons-logging 实现（版本低，出现方法找不到问题） -->
            <exclusion>
                <groupId>org.springframework</groupId>
                <artifactId>spring-jcl</artifactId>
            </exclusion>
        </exclusions>
    </dependency>
    
    <!-- 与 logback 整合（通过 @Slf4j 注解即可使用） -->
    <dependency>
        <groupId>org.projectlombok</groupId>
        <artifactId>lombok</artifactId>
        <version>1.18.10</version>
    </dependency>
    <!--引入log4j日志依赖，目的是使用 jcl-over-slf4j 来重写 commons logging 的实现-->
    <dependency>
      <groupId>org.springframework.boot</groupId>
      <artifactId>spring-boot-starter-log4j</artifactId>
      <version>1.3.8.RELEASE</version>
      <exclusions>
        <exclusion>
          <artifactId>slf4j-log4j12</artifactId>
          <groupId>org.slf4j</groupId>
        </exclusion>
      </exclusions>
    </dependency>
</dependencies>

```

---- 

### 7. 高可用集群

```properties
cn.fyupeng.nacos.cluster.use=true
cn.fyupeng.nacos.cluster.load-balancer=random
cn.fyupeng.nacos.cluster.nodes=192.168.10.1:8847,192.168.10.1:8848,192.168.10.1:8849
```
- 使用方法及注意要点

默认情况下，`cn.fyupeng.nacos.cluster.use`服从约定优于配置设定，且默认值为`false`，表示默认不打开集群；

`cn.fyupeng.nacos.cluster.load-balancer`与`cn.fyupeng.nacos.cluster.nodes`使用前必须先打开集群模式

- 集群节点负载策略
    - `random`：随机策略
    - `round`：轮询策略

集群节点理论上允许无限扩展，可使用分隔符`[;,|]`扩展配置

- 集群节点容错切换
    - 节点宕机：遇到节点宕机将重新从节点配置列表中选举新的正常节点，否则无限重试

---- 

### 8. 超时重试机制

默认不使用重试机制，为了保证服务的正确性，因为无法保证幂等性。

原因是客户端无法探测是客户端网络传输过程出现问题，或者是服务端正确接收后返回途中网络传输出现问题，因为如果是前者那么重试后能保证幂等性，如果为后者，可能将导致多次同个业务的执行，这对客户端来说结果是非一致的。

超时重试机制会导致出现幂等性问题，因此在客户端请求包中加入重发标志位表明重发包、在服务器中使用`HashSet`添加请求`id`来做超时缓存处理

- 超时重试：`cn.fyupeng.annotation.Reference`注解提供重试次数、超时时间、异步时间和重试让出多个配置参数，其中：
    - 重试次数：服务端未能在超时时间内 响应，允许触发超时的次数
    - 超时时间：即客户端最长允许等待 服务端时长，超时即触发重试机制
    - 异步时间：即等待服务端异步响应的时间，且只能在超时重试机制使用，非超时重试情况下默认使用阻塞等待方式
    - 重试让出时间：默认为 1 秒，降低线程调度cpu资源竞争，能够解决`cpu`吞吐量低下问题

> 示例：
```java
private static RandomLoadBalancer randomLoadBalancer = new RandomLoadBalancer();
    private static NettyClient nettyClient = new NettyClient(randomLoadBalancer, CommonSerializer.KRYO_SERIALIZER);
    private static RpcClientProxy rpcClientProxy = new RpcClientProxy(nettyClient);

    @Reference(retries = 2, timeout = 1000, asyncTime = 3000, giveTime = 1)
    private static HelloWorldService service = rpcClientProxy.getProxy(HelloWorldService.class, Client.class);
```

- 幂等性

重试机制服务端要保证重试包的一次执行原则，即要实现幂等性。

实现思路：借助分布式缓存对首次请求包进行异步缓存请求`id`，分布式缓存选型`Redis`，客户端选型`Jedis与Lettuce`，给定一个`key`失效时间满足重试机制，减少再次清理缓存的步骤。

幂等性作用在失效时间，也影响到超时机制重试次数以及高并发场景，有雪花算法的优势，不同时间上不会生成重复id，于是可以大胆将失效时间设置大些，这取决于你能够承担多少内存而转而去考虑内存瓶颈的问题。

---- 

### 9. 雪花算法
```java
/**
     * 自定义 分布式唯一号 id
     *  1 位 符号位
     * 41 位 时间戳
     * 10 位 工作机器 id
     * 12 位 并发序列号
     *
     *       The distribute unique id is as follows :
     * +---------------++---------------+---------------+-----------------+
     * |     Sign      |     epoch     |    workerId   |     sequence     |
     * |    1 bits     |    41 bits    |    10 bits   |      12 bits      |
     * +---------------++---------------+---------------+-----------------+
     */
```
- 介绍

雪花算法：主要由时间戳、机器码、序列码这三者组成，各个部分有代表的意义。

> 标志位

表示符号位，固定值为`0`，表示正数。

> 时间戳

时间戳代表运行时长，它由`41`比特组成，最高可使用69年，由当前系统时间戳与服务启动当天凌晨时间戳的差值表示。

> 机器码

机器码代表分布式节点，它由`10`比特组成，最高可表示`1024`个机器码，默认采用当前服务主机号`hashCode`、高低位异或和分布式缓存算法生成，机器码生成异常时由`SecureRandom`使用随机种子、`AtomicLong`与`CAS`锁生成，当机器码数量最大时将抛出异常`WorkerIdCantApplyException`。

> 序列号

序列号代码并发量，在同一毫秒内发生作用，即毫秒并发时作为一部分唯一值，它由`12`比特组成，最高可表示`4096`个序列号。

- 应用场景

（1）多服务节点负载实现业务持久化主键唯一

（2）毫秒内请求并发数最高可达到`4095`，可适当改变序列号满足不同场景

（3）满足基本有序的唯一号`id`，有利于高效索引查询和维护

（4）`id`号前`6`位附加当前时间年月日日志查询，可作为日志记录

而在`RPC`中主要采用雪花算法实现了请求包的唯一识别号，因为`UUID`生成唯一性和时间持续性比雪花算法更好，但它id值是非递增序列，在索引建立和维护时代价更高。

雪花算法生成的`id`基本有序递增，具有分布式唯一性，而且维护成本低，可作为主键或其他索引值，代价是强依赖机器时钟，服务器重启必须手动设置系统时间同步。
```properties
cn.fyupeng.redis.server-addr=127.0.0.1:6379
cn.fyupeng.redis.server-auth=true
cn.fyupeng.redis.server-pwd=123456
```
除此以外，超时重试机制，在分布式场景下，第二次重试会通过负载均衡策略负载到其他服务节点，利用雪花算法弥补了分布式场景下的无法解决幂等性的问题。

超时重试采用`Jedis/Lettuce`两种实现缓存的方式，可分别在服务端、客户端配置相应的缓存连接客户端方式：
```properties
cn.fyupeng.redis.server-way=lettuce
cn.fyupeng.redis.client-way=jedis
cn.fyupeng.redis.server-async=true
```
如何选择`JRedis`与`LRedis`呢？

JRedis
- 线程安全
- `synchronized`与`lock`的悲观锁机制
- 不提供线程池
- 连接数为`1`
- 同步操作
- 提供依据主机号缓存和获取机器`id`
- 提供依据请求`id`缓存和获取请求结果

LRedis
- 线程安全
- 提供线程池
- 连接数稳定且由线程池提供
- 异步/同步操作
- 提供依据主机号缓存和获取机器`id`
- 提供依据请求`id`缓存和获取请求结果

>特别提醒

高并发请求不会出现请求号重复的情况，当前最高毫秒级并发`4096`，而超时机制、`LRedis`线程池对连接的超时控制等配置参数还不成熟，具体应用场景可自行下载源码修改参数。

---- 

### <a id="highConcurrency">10. 高并发</a>

在`Netty`高性能框架的支持下，有单`Reactor`单线程、单`Reactor`多线程和主从`Reactor`多线程，采用性能最好的主从`Reactor`多线程，优势在于多服务结点（多`channel`情况下）并发处理时，从`workGroup`中每个线程可以处理一个`channel`，实现并行处理，不过在单个`channel`承载高并发下，无法多个线程同时处理事件，因为一个`channel`只能绑定一个线程。

如果多个线程同时处理一个`channel`，将会出现类似`Redis`多线程情况下，用`Jedis`操作出现的安全问题，这里因为多个线程应对一个`channe`l将会使情况变得异常复杂，这里跟`Redis`单线程一样，异曲同工，速度之快在于单线程不用考虑多线程之间的协调性，只要再次分发`channel`到线程池中执行，那个获取到`channel`的线程就可以去读取消息，自行写入返回消息给服务端即可。

这样负责读取`channel`绑定的单线程只需要提交任务到线程池，不需阻塞，即可处理高并发请求。

从代码细节上来讲，读取到`channel`事件，意味着缓存`buffer`已经被读走了，不会影响其他线程继续读取`channel`，当然这里会出现短暂的阻塞，因为读取也需要一定时间，所以不会出现多个任务提交执行出现交叉行为。

这里读取之快又涉及到零拷贝，数据在用户态是不用拷贝的，直接透明使用。


当然高并发下还要考虑一个问题，任务处理太慢时，不能让客户端一直阻塞等待，可以设置超时，避免因为服务端某一个任务影响到其他请求的执行，要让出给其他有需要的线程使用，于是引入的超时机制配合分布式缓存，在超时机制下，要么直接将第一次请求后服务端缓存的结果直接返回，要么直接失败，来保证它的一个高并发稳定性。

